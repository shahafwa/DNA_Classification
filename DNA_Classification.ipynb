{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNA_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mpU7viIT31z"
      },
      "source": [
        "In case we don't have Bio libary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-UdbvCSwHM"
      },
      "source": [
        "# !pip install Bio"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qQS6Y5lT7ml"
      },
      "source": [
        "Essential libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_ivVNCBRhg0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import itertools\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from captum.attr import IntegratedGradients\n",
        "# from captum.attr import LayerConductance\n",
        "# from captum.attr import NeuronConductance\n",
        "from Bio import SeqIO, SeqUtils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvRLPruXT9jH"
      },
      "source": [
        "Calculation of DNA Codon usage, kmer usage, orf and general DNA features for usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FCeO_M6Se7M"
      },
      "source": [
        "# codon usage features extraction\n",
        "codons = [''.join(i) for i in list(itertools.product(['A', 'G', 'T', 'C'], repeat=3) )]\n",
        "\n",
        "\n",
        "def calculate_codon_usage(seq):\n",
        "    vector = []\n",
        "    length = len(seq)\n",
        "    a = Counter([str(seq[3*i:3*i+3]) for i in range(0, len(seq)//3)])\n",
        "    b = {i:a[i] / length for i in a}\n",
        "    for cod in codons:\n",
        "        if cod in b:\n",
        "            vector.append(b[cod])\n",
        "        else:\n",
        "            vector.append(0)\n",
        "    return vector\n",
        "\n",
        "def calculate_kmer_usage(seq, kmers_length):\n",
        "    KMERS = [''.join(i) for i in list(itertools.product(['A', 'T', 'G', 'C'], repeat=kmers_length) )]\n",
        "    vector = []\n",
        "    length = len(seq)\n",
        "    a = Counter([str(seq[i:i+kmers_length]) for i in range(0, len(seq) - kmers_length)])\n",
        "    b = {i:a[i] / length for i in a}\n",
        "    for kmer in KMERS:\n",
        "        if kmer in b:\n",
        "            vector.append(b[kmer])\n",
        "        else:\n",
        "            vector.append(0)\n",
        "    return vector\n",
        "\n",
        "# total ORF length feature extraction\n",
        "def get_orfs_len(seq):\n",
        "    orfs = [i[0] for i in re.findall(\"((ATG|GTG|TTG).{100,}?(TAG|TAA|TGA))\", seq) if len(i[0]) % 3 == 0]\n",
        "    return sum([len(i) for i in orfs]) / len(seq)\n",
        "\n",
        "def extract_data_features(fasta_file):\n",
        "    fasta = list(SeqIO.parse(fasta_file,  \"fasta\"))\n",
        "    df = []\n",
        "    for f in fasta:\n",
        "        df.append((f.id, ''.join(f.seq)))\n",
        "    df = pd.DataFrame(df, columns=['id', 'seq'])\n",
        "    # extract_features\n",
        "    df['len'] = df.seq.str.len()\n",
        "    df['classification'] = df.id.str.split('-').str[0].str.replace('Bacteria', '0').str.replace('Phage', '1').astype(int)\n",
        "    df['gc_content'] = df.seq.apply(SeqUtils.GC)\n",
        "    df['ATG_freq'] = df.seq.str.count('ATG') / df.len # redundant with codon usage\n",
        "    df['GTG_freqs'] = df.seq.str.count('GTG') / df.len # redundant with codon usage\n",
        "    df['TTG_freq'] = df.seq.str.count('TTG') / df.len # redundant with codon usage\n",
        "    df['A_freq'] = df.seq.str.count('A') / df.len\n",
        "    df['G_freq'] = df.seq.str.count('G') / df.len\n",
        "    df['C_freq'] = df.seq.str.count('C') / df.len\n",
        "    df['T_freq'] = df.seq.str.count('T') / df.len # redundant\n",
        "    df['extra_orfs_length'] = df.seq.apply(get_orfs_len)\n",
        "    \n",
        "    df['codon_usage_vec'] = df.seq.apply(calculate_codon_usage)\n",
        "    df = pd.merge(df, df.codon_usage_vec.apply(pd.Series), right_index=True, left_index=True)\n",
        "    df = df.drop(columns=['codon_usage_vec'])\n",
        "    \n",
        "    for i in [2, 3, 4]:\n",
        "        df['kmer_usage_vec'] = df.seq.apply(lambda x: calculate_kmer_usage(x,i))\n",
        "        df = pd.merge(df, df.kmer_usage_vec.apply(pd.Series), right_index=True, left_index=True)\n",
        "        df = df.drop(columns=['kmer_usage_vec'])\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkAzZfyuUKhm"
      },
      "source": [
        "Load the data and display it with the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "bSroskFKGF1j",
        "outputId": "328682d9-e5bd-418b-9865-711ac3822f18"
      },
      "source": [
        "# features for data\n",
        "df_train = extract_data_features(\"train.fasta\")\n",
        "df_train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>seq</th>\n",
              "      <th>len</th>\n",
              "      <th>classification</th>\n",
              "      <th>gc_content</th>\n",
              "      <th>ATG_freq</th>\n",
              "      <th>GTG_freqs</th>\n",
              "      <th>TTG_freq</th>\n",
              "      <th>A_freq</th>\n",
              "      <th>G_freq</th>\n",
              "      <th>C_freq</th>\n",
              "      <th>T_freq</th>\n",
              "      <th>extra_orfs_length</th>\n",
              "      <th>0_x</th>\n",
              "      <th>1_x</th>\n",
              "      <th>2_x</th>\n",
              "      <th>3_x</th>\n",
              "      <th>4_x</th>\n",
              "      <th>5_x</th>\n",
              "      <th>6_x</th>\n",
              "      <th>7_x</th>\n",
              "      <th>8_x</th>\n",
              "      <th>9_x</th>\n",
              "      <th>10_x</th>\n",
              "      <th>11_x</th>\n",
              "      <th>12_x</th>\n",
              "      <th>13_x</th>\n",
              "      <th>14_x</th>\n",
              "      <th>15_x</th>\n",
              "      <th>16_x</th>\n",
              "      <th>17_x</th>\n",
              "      <th>18_x</th>\n",
              "      <th>19_x</th>\n",
              "      <th>20_x</th>\n",
              "      <th>21_x</th>\n",
              "      <th>22_x</th>\n",
              "      <th>23_x</th>\n",
              "      <th>24_x</th>\n",
              "      <th>25_x</th>\n",
              "      <th>26_x</th>\n",
              "      <th>...</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bacteria-4999</td>\n",
              "      <td>ATGAATCCAAGCCAAATACTTGAAAATTTAAAAAAAGAATTAAACG...</td>\n",
              "      <td>1323</td>\n",
              "      <td>0</td>\n",
              "      <td>30.234316</td>\n",
              "      <td>0.025699</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.012094</td>\n",
              "      <td>0.424792</td>\n",
              "      <td>0.145881</td>\n",
              "      <td>0.156463</td>\n",
              "      <td>0.272865</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.036281</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.018896</td>\n",
              "      <td>0.011338</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.021920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014361</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.003779</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bacteria-5000</td>\n",
              "      <td>ATGAAGTTAAGTATCAATAAAAATACTTTAGAATCTGCAGTGATTT...</td>\n",
              "      <td>1068</td>\n",
              "      <td>0</td>\n",
              "      <td>26.310861</td>\n",
              "      <td>0.023408</td>\n",
              "      <td>0.006554</td>\n",
              "      <td>0.014045</td>\n",
              "      <td>0.421348</td>\n",
              "      <td>0.128277</td>\n",
              "      <td>0.134831</td>\n",
              "      <td>0.315543</td>\n",
              "      <td>0.308989</td>\n",
              "      <td>0.036517</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.015918</td>\n",
              "      <td>0.009363</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012172</td>\n",
              "      <td>0.004682</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.009363</td>\n",
              "      <td>0.015918</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>0.007491</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.028090</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.016854</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.005618</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.004682</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.006554</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bacteria-5001</td>\n",
              "      <td>ATGCAAGAAAATTACGGTGCGAGTAATATTAAAGTCCTAAAAGGCC...</td>\n",
              "      <td>2310</td>\n",
              "      <td>0</td>\n",
              "      <td>32.987013</td>\n",
              "      <td>0.024675</td>\n",
              "      <td>0.016017</td>\n",
              "      <td>0.021212</td>\n",
              "      <td>0.374892</td>\n",
              "      <td>0.199134</td>\n",
              "      <td>0.130736</td>\n",
              "      <td>0.295238</td>\n",
              "      <td>0.341558</td>\n",
              "      <td>0.025108</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.016450</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.006926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006926</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.007359</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.007792</td>\n",
              "      <td>0.004762</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.030736</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.019048</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.009957</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.005628</td>\n",
              "      <td>0.006061</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.003463</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.004329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bacteria-5002</td>\n",
              "      <td>GTGCTGGCCGAGGAGTTGACGTCCTATCGCGGCACCGACGAGCTGC...</td>\n",
              "      <td>603</td>\n",
              "      <td>0</td>\n",
              "      <td>69.651741</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.144279</td>\n",
              "      <td>0.371476</td>\n",
              "      <td>0.325041</td>\n",
              "      <td>0.159204</td>\n",
              "      <td>0.592040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.019900</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.013267</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.018242</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.016584</td>\n",
              "      <td>0.026534</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.018242</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.024876</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016584</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.009950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bacteria-5003</td>\n",
              "      <td>ATGCCGCTTACACCAGCCGATGTCCACAATGTGGCCTTCAGCAAGC...</td>\n",
              "      <td>819</td>\n",
              "      <td>0</td>\n",
              "      <td>69.841270</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.199023</td>\n",
              "      <td>0.315018</td>\n",
              "      <td>0.383394</td>\n",
              "      <td>0.102564</td>\n",
              "      <td>0.161172</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.009768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.012210</td>\n",
              "      <td>0.023199</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.009768</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.013431</td>\n",
              "      <td>0.029304</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.023199</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.024420</td>\n",
              "      <td>0.009768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>0.002442</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 413 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  ...       255\n",
              "0  Bacteria-4999  ...  0.000000\n",
              "1  Bacteria-5000  ...  0.000000\n",
              "2  Bacteria-5001  ...  0.000000\n",
              "3  Bacteria-5002  ...  0.009950\n",
              "4  Bacteria-5003  ...  0.002442\n",
              "\n",
              "[5 rows x 413 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb4WpUIHURX-"
      },
      "source": [
        "Drop data which isn't essential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "Z8x0YR-KJtWB",
        "outputId": "c8ba001e-6564-4d33-b8ed-8d9342cda29e"
      },
      "source": [
        "data = df_train.drop(columns=['id', 'seq', 'len'])\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classification</th>\n",
              "      <th>gc_content</th>\n",
              "      <th>ATG_freq</th>\n",
              "      <th>GTG_freqs</th>\n",
              "      <th>TTG_freq</th>\n",
              "      <th>A_freq</th>\n",
              "      <th>G_freq</th>\n",
              "      <th>C_freq</th>\n",
              "      <th>T_freq</th>\n",
              "      <th>extra_orfs_length</th>\n",
              "      <th>0_x</th>\n",
              "      <th>1_x</th>\n",
              "      <th>2_x</th>\n",
              "      <th>3_x</th>\n",
              "      <th>4_x</th>\n",
              "      <th>5_x</th>\n",
              "      <th>6_x</th>\n",
              "      <th>7_x</th>\n",
              "      <th>8_x</th>\n",
              "      <th>9_x</th>\n",
              "      <th>10_x</th>\n",
              "      <th>11_x</th>\n",
              "      <th>12_x</th>\n",
              "      <th>13_x</th>\n",
              "      <th>14_x</th>\n",
              "      <th>15_x</th>\n",
              "      <th>16_x</th>\n",
              "      <th>17_x</th>\n",
              "      <th>18_x</th>\n",
              "      <th>19_x</th>\n",
              "      <th>20_x</th>\n",
              "      <th>21_x</th>\n",
              "      <th>22_x</th>\n",
              "      <th>23_x</th>\n",
              "      <th>24_x</th>\n",
              "      <th>25_x</th>\n",
              "      <th>26_x</th>\n",
              "      <th>27_x</th>\n",
              "      <th>28_x</th>\n",
              "      <th>29_x</th>\n",
              "      <th>...</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>30.234316</td>\n",
              "      <td>0.025699</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.012094</td>\n",
              "      <td>0.424792</td>\n",
              "      <td>0.145881</td>\n",
              "      <td>0.156463</td>\n",
              "      <td>0.272865</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.036281</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.018896</td>\n",
              "      <td>0.011338</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>0.016629</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.021920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014361</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.007559</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.008314</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.003023</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.003779</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005291</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>26.310861</td>\n",
              "      <td>0.023408</td>\n",
              "      <td>0.006554</td>\n",
              "      <td>0.014045</td>\n",
              "      <td>0.421348</td>\n",
              "      <td>0.128277</td>\n",
              "      <td>0.134831</td>\n",
              "      <td>0.315543</td>\n",
              "      <td>0.308989</td>\n",
              "      <td>0.036517</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.015918</td>\n",
              "      <td>0.009363</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012172</td>\n",
              "      <td>0.004682</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.009363</td>\n",
              "      <td>0.015918</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>0.007491</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.028090</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.016854</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.005618</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.004682</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.006554</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003745</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.001873</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>32.987013</td>\n",
              "      <td>0.024675</td>\n",
              "      <td>0.016017</td>\n",
              "      <td>0.021212</td>\n",
              "      <td>0.374892</td>\n",
              "      <td>0.199134</td>\n",
              "      <td>0.130736</td>\n",
              "      <td>0.295238</td>\n",
              "      <td>0.341558</td>\n",
              "      <td>0.025108</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.016450</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.006926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006926</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.007359</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.007792</td>\n",
              "      <td>0.004762</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.030736</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.019048</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.009957</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.005628</td>\n",
              "      <td>0.006061</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.008658</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>0.003463</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.002597</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.004329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>69.651741</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.144279</td>\n",
              "      <td>0.371476</td>\n",
              "      <td>0.325041</td>\n",
              "      <td>0.159204</td>\n",
              "      <td>0.592040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.019900</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.013267</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.018242</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.016584</td>\n",
              "      <td>0.026534</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.018242</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.024876</td>\n",
              "      <td>0.009950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016584</td>\n",
              "      <td>0.006633</td>\n",
              "      <td>0.004975</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.008292</td>\n",
              "      <td>0.009950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>69.841270</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.199023</td>\n",
              "      <td>0.315018</td>\n",
              "      <td>0.383394</td>\n",
              "      <td>0.102564</td>\n",
              "      <td>0.161172</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.009768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.012210</td>\n",
              "      <td>0.023199</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.019536</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.012210</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.009768</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.013431</td>\n",
              "      <td>0.029304</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.018315</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.023199</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.008547</td>\n",
              "      <td>0.024420</td>\n",
              "      <td>0.009768</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>0.002442</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 410 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   classification  gc_content  ATG_freq  ...       253       254       255\n",
              "0               0   30.234316  0.025699  ...  0.000000  0.000000  0.000000\n",
              "1               0   26.310861  0.023408  ...  0.000936  0.000000  0.000000\n",
              "2               0   32.987013  0.024675  ...  0.000000  0.000000  0.000000\n",
              "3               0   69.651741  0.008292  ...  0.001658  0.008292  0.009950\n",
              "4               0   69.841270  0.007326  ...  0.000000  0.014652  0.002442\n",
              "\n",
              "[5 rows x 410 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-AiuqdpUUnW"
      },
      "source": [
        "Scale the data and split label and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_F1T4QAKLLA"
      },
      "source": [
        "data_X = data.loc[:, data.columns != 'classification']\n",
        "data_y = data.loc[:, data.columns == 'classification'].to_numpy()\n",
        "scaler = StandardScaler()\n",
        "data_X = scaler.fit_transform(data_X)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6nNo3V2UXzr"
      },
      "source": [
        "Create a Pytorch Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7_MKwvELYOp"
      },
      "source": [
        "class DNAData(Dataset):\n",
        "    \"\"\"DNA dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        item = self.data[idx]\n",
        "        item_label = self.labels[idx]\n",
        "\n",
        "\n",
        "        return torch.Tensor(item), torch.Tensor(item_label)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaWlo_8VUoqN"
      },
      "source": [
        "Create the dataloaders and datasets with split data for train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVRQmhZSLnGR"
      },
      "source": [
        "DNA_data = DNAData(data_X, data_y)\n",
        "l_DNA = len(DNA_data)\n",
        "train_set, val_set = torch.utils.data.random_split(DNA_data, [int(l_DNA*0.8), l_DNA- int(l_DNA*0.8)])\n",
        "\n",
        "dataloaders =  {'train': DataLoader(train_set, batch_size=64, shuffle=True),\n",
        "                  'val': DataLoader(val_set, batch_size=64, shuffle=True)}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Hwg8ULUvHk"
      },
      "source": [
        "Create a Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3HboWdaQI6e"
      },
      "source": [
        "class DNA_model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super(DNA_model, self).__init__()\n",
        "      self.linear0 = nn.Linear(data_X.shape[1], 128)\n",
        "      self.bn0 = nn.BatchNorm1d(128)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.linear1 = nn.Linear(128, 64)\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.linear2 = nn.Linear(64, 32)\n",
        "      self.bn2 = nn.BatchNorm1d(32)\n",
        "      self.linear3 = nn.Linear(32, 8)\n",
        "      self.bn3 = nn.BatchNorm1d(8)\n",
        "      self.linear4 = nn.Linear(8, 1)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "      # nn.BatchNorm1d(int(hidden_number/2))\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.linear0(x)\n",
        "      x = self.bn0(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.linear1(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.linear2(x)\n",
        "      x = self.bn2(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.linear3(x)\n",
        "      x = self.bn3(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.linear4(x)\n",
        "      x = self.sigmoid(x)\n",
        "      return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvUzqlK_UzVc"
      },
      "source": [
        "Create the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97BhZL40QJ_Q"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    trg = []\n",
        "    oup = []\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        \n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            count = 0\n",
        "            \n",
        "\n",
        "            # Iterate over data.\n",
        "            for inp, labels in dataloaders[phase]:\n",
        "                inp = inp.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inp)\n",
        "                    preds = torch.round(outputs)\n",
        "                    loss = criterion(outputs, labels.float())\n",
        "                    count += len(inp)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inp.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                trg.append(labels.detach())\n",
        "                oup.append(preds.detach())\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / (len(dataloaders[phase])*150)\n",
        "            epoch_acc = running_corrects.double() / (len(dataloaders[phase])*150)\n",
        "            it = [item for sublist in trg for item in sublist]\n",
        "            tr = torch.tensor(it)\n",
        "            ot = [item for sublist in oup for item in sublist]\n",
        "            prd = torch.tensor(ot)\n",
        "\n",
        "            epoch_f1 = f1_score(tr, prd, average='macro')\n",
        "            epoch_precision = precision_score(tr, prd, average='macro')\n",
        "            epoch_recall = recall_score(tr, prd, average='macro')\n",
        "            epoch_accuracy = accuracy_score(tr, prd)\n",
        "\n",
        "            # epoch_f1 = 0\n",
        "            # epoch_precision = 0\n",
        "            # epoch_recall = 0\n",
        "\n",
        "\n",
        "            trg = []\n",
        "            oup = []\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f} F1 score: {:.4f} Precision: {:.4f} Recall: {:.4f} '.format(\n",
        "                phase, epoch_loss, epoch_accuracy, epoch_f1, epoch_precision, epoch_recall))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_f1 > best_f1:\n",
        "                best_f1 = epoch_f1\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val F1: {:4f}'.format(best_f1))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ3Seu1bU1hK"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjzK5BuuRkTI",
        "outputId": "59592810-633d-48a1-fa16-9246a9c7fd51"
      },
      "source": [
        "model = DNA_model()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "lr=0.005\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "model = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.1877 Acc: 0.8020 F1 score: 0.8019 Precision: 0.8020 Recall: 0.8019 \n",
            "val Loss: 0.1712 Acc: 0.8160 F1 score: 0.8160 Precision: 0.8164 Recall: 0.8162 \n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.1394 Acc: 0.8560 F1 score: 0.8560 Precision: 0.8560 Recall: 0.8560 \n",
            "val Loss: 0.1418 Acc: 0.8585 F1 score: 0.8585 Precision: 0.8591 Recall: 0.8588 \n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.1237 Acc: 0.8746 F1 score: 0.8746 Precision: 0.8746 Recall: 0.8746 \n",
            "val Loss: 0.1297 Acc: 0.8720 F1 score: 0.8719 Precision: 0.8739 Recall: 0.8725 \n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.1078 Acc: 0.8928 F1 score: 0.8928 Precision: 0.8929 Recall: 0.8928 \n",
            "val Loss: 0.1206 Acc: 0.8800 F1 score: 0.8800 Precision: 0.8806 Recall: 0.8803 \n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.0998 Acc: 0.9011 F1 score: 0.9011 Precision: 0.9013 Recall: 0.9011 \n",
            "val Loss: 0.1193 Acc: 0.8850 F1 score: 0.8850 Precision: 0.8850 Recall: 0.8851 \n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.0942 Acc: 0.9061 F1 score: 0.9061 Precision: 0.9064 Recall: 0.9061 \n",
            "val Loss: 0.1234 Acc: 0.8875 F1 score: 0.8875 Precision: 0.8879 Recall: 0.8877 \n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.0757 Acc: 0.9264 F1 score: 0.9264 Precision: 0.9264 Recall: 0.9263 \n",
            "val Loss: 0.1293 Acc: 0.8820 F1 score: 0.8820 Precision: 0.8823 Recall: 0.8822 \n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.0551 Acc: 0.9491 F1 score: 0.9491 Precision: 0.9491 Recall: 0.9491 \n",
            "val Loss: 0.1184 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8957 Recall: 0.8957 \n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.0479 Acc: 0.9537 F1 score: 0.9537 Precision: 0.9538 Recall: 0.9537 \n",
            "val Loss: 0.1220 Acc: 0.8960 F1 score: 0.8960 Precision: 0.8960 Recall: 0.8961 \n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0434 Acc: 0.9584 F1 score: 0.9584 Precision: 0.9584 Recall: 0.9583 \n",
            "val Loss: 0.1239 Acc: 0.8945 F1 score: 0.8945 Precision: 0.8945 Recall: 0.8945 \n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0388 Acc: 0.9650 F1 score: 0.9650 Precision: 0.9650 Recall: 0.9650 \n",
            "val Loss: 0.1320 Acc: 0.8920 F1 score: 0.8920 Precision: 0.8923 Recall: 0.8922 \n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0355 Acc: 0.9687 F1 score: 0.9687 Precision: 0.9688 Recall: 0.9687 \n",
            "val Loss: 0.1321 Acc: 0.8980 F1 score: 0.8980 Precision: 0.8980 Recall: 0.8979 \n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0316 Acc: 0.9729 F1 score: 0.9729 Precision: 0.9729 Recall: 0.9729 \n",
            "val Loss: 0.1441 Acc: 0.8905 F1 score: 0.8905 Precision: 0.8912 Recall: 0.8908 \n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0280 Acc: 0.9759 F1 score: 0.9759 Precision: 0.9759 Recall: 0.9759 \n",
            "val Loss: 0.1451 Acc: 0.8990 F1 score: 0.8990 Precision: 0.8993 Recall: 0.8992 \n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0239 Acc: 0.9791 F1 score: 0.9791 Precision: 0.9791 Recall: 0.9791 \n",
            "val Loss: 0.1446 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8970 Recall: 0.8970 \n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 0.9805 F1 score: 0.9805 Precision: 0.9805 Recall: 0.9805 \n",
            "val Loss: 0.1445 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8970 Recall: 0.8971 \n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0251 Acc: 0.9780 F1 score: 0.9780 Precision: 0.9780 Recall: 0.9780 \n",
            "val Loss: 0.1459 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8956 Recall: 0.8957 \n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0235 Acc: 0.9799 F1 score: 0.9799 Precision: 0.9799 Recall: 0.9799 \n",
            "val Loss: 0.1455 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8955 Recall: 0.8956 \n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0232 Acc: 0.9795 F1 score: 0.9795 Precision: 0.9795 Recall: 0.9795 \n",
            "val Loss: 0.1490 Acc: 0.8950 F1 score: 0.8950 Precision: 0.8954 Recall: 0.8953 \n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0226 Acc: 0.9815 F1 score: 0.9815 Precision: 0.9815 Recall: 0.9815 \n",
            "val Loss: 0.1463 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8971 Recall: 0.8971 \n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0221 Acc: 0.9810 F1 score: 0.9810 Precision: 0.9810 Recall: 0.9810 \n",
            "val Loss: 0.1516 Acc: 0.8945 F1 score: 0.8945 Precision: 0.8952 Recall: 0.8948 \n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0229 Acc: 0.9809 F1 score: 0.9809 Precision: 0.9809 Recall: 0.9809 \n",
            "val Loss: 0.1450 Acc: 0.8935 F1 score: 0.8935 Precision: 0.8935 Recall: 0.8935 \n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0236 Acc: 0.9789 F1 score: 0.9789 Precision: 0.9789 Recall: 0.9789 \n",
            "val Loss: 0.1498 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8959 Recall: 0.8957 \n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0216 Acc: 0.9830 F1 score: 0.9830 Precision: 0.9830 Recall: 0.9830 \n",
            "val Loss: 0.1458 Acc: 0.8940 F1 score: 0.8940 Precision: 0.8940 Recall: 0.8940 \n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0218 Acc: 0.9829 F1 score: 0.9829 Precision: 0.9829 Recall: 0.9829 \n",
            "val Loss: 0.1469 Acc: 0.9000 F1 score: 0.9000 Precision: 0.9001 Recall: 0.9001 \n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0227 Acc: 0.9807 F1 score: 0.9807 Precision: 0.9807 Recall: 0.9807 \n",
            "val Loss: 0.1465 Acc: 0.8990 F1 score: 0.8990 Precision: 0.8990 Recall: 0.8991 \n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0235 Acc: 0.9791 F1 score: 0.9791 Precision: 0.9791 Recall: 0.9791 \n",
            "val Loss: 0.1477 Acc: 0.8965 F1 score: 0.8965 Precision: 0.8965 Recall: 0.8966 \n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0198 Acc: 0.9851 F1 score: 0.9851 Precision: 0.9851 Recall: 0.9851 \n",
            "val Loss: 0.1464 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8970 Recall: 0.8970 \n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0218 Acc: 0.9819 F1 score: 0.9819 Precision: 0.9819 Recall: 0.9819 \n",
            "val Loss: 0.1458 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8970 Recall: 0.8971 \n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 0.9816 F1 score: 0.9816 Precision: 0.9816 Recall: 0.9816 \n",
            "val Loss: 0.1462 Acc: 0.8960 F1 score: 0.8960 Precision: 0.8960 Recall: 0.8960 \n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 0.9821 F1 score: 0.9821 Precision: 0.9821 Recall: 0.9821 \n",
            "val Loss: 0.1470 Acc: 0.8960 F1 score: 0.8960 Precision: 0.8960 Recall: 0.8960 \n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0222 Acc: 0.9820 F1 score: 0.9820 Precision: 0.9820 Recall: 0.9820 \n",
            "val Loss: 0.1465 Acc: 0.8945 F1 score: 0.8945 Precision: 0.8945 Recall: 0.8945 \n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0225 Acc: 0.9816 F1 score: 0.9816 Precision: 0.9816 Recall: 0.9816 \n",
            "val Loss: 0.1466 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8957 Recall: 0.8957 \n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0211 Acc: 0.9837 F1 score: 0.9837 Precision: 0.9837 Recall: 0.9837 \n",
            "val Loss: 0.1477 Acc: 0.8990 F1 score: 0.8990 Precision: 0.8991 Recall: 0.8991 \n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0219 Acc: 0.9814 F1 score: 0.9814 Precision: 0.9814 Recall: 0.9814 \n",
            "val Loss: 0.1472 Acc: 0.8945 F1 score: 0.8945 Precision: 0.8945 Recall: 0.8945 \n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0232 Acc: 0.9815 F1 score: 0.9815 Precision: 0.9815 Recall: 0.9815 \n",
            "val Loss: 0.1513 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8957 Recall: 0.8957 \n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0211 Acc: 0.9834 F1 score: 0.9834 Precision: 0.9834 Recall: 0.9834 \n",
            "val Loss: 0.1499 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8973 Recall: 0.8972 \n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0216 Acc: 0.9827 F1 score: 0.9827 Precision: 0.9827 Recall: 0.9827 \n",
            "val Loss: 0.1446 Acc: 0.8965 F1 score: 0.8965 Precision: 0.8965 Recall: 0.8965 \n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0219 Acc: 0.9814 F1 score: 0.9814 Precision: 0.9814 Recall: 0.9814 \n",
            "val Loss: 0.1470 Acc: 0.8955 F1 score: 0.8955 Precision: 0.8955 Recall: 0.8954 \n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.0230 Acc: 0.9821 F1 score: 0.9821 Precision: 0.9821 Recall: 0.9821 \n",
            "val Loss: 0.1472 Acc: 0.8985 F1 score: 0.8985 Precision: 0.8986 Recall: 0.8986 \n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0231 Acc: 0.9806 F1 score: 0.9806 Precision: 0.9806 Recall: 0.9806 \n",
            "val Loss: 0.1488 Acc: 0.8965 F1 score: 0.8965 Precision: 0.8967 Recall: 0.8967 \n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0211 Acc: 0.9831 F1 score: 0.9831 Precision: 0.9831 Recall: 0.9831 \n",
            "val Loss: 0.1468 Acc: 0.8940 F1 score: 0.8940 Precision: 0.8940 Recall: 0.8940 \n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0215 Acc: 0.9824 F1 score: 0.9824 Precision: 0.9824 Recall: 0.9824 \n",
            "val Loss: 0.1486 Acc: 0.8970 F1 score: 0.8970 Precision: 0.8971 Recall: 0.8971 \n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.0221 Acc: 0.9831 F1 score: 0.9831 Precision: 0.9831 Recall: 0.9831 \n",
            "val Loss: 0.1478 Acc: 0.8985 F1 score: 0.8985 Precision: 0.8986 Recall: 0.8986 \n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.0221 Acc: 0.9814 F1 score: 0.9814 Precision: 0.9814 Recall: 0.9814 \n",
            "val Loss: 0.1495 Acc: 0.8980 F1 score: 0.8980 Precision: 0.8982 Recall: 0.8982 \n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 0.9794 F1 score: 0.9794 Precision: 0.9794 Recall: 0.9794 \n",
            "val Loss: 0.1491 Acc: 0.8980 F1 score: 0.8980 Precision: 0.8982 Recall: 0.8982 \n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0203 Acc: 0.9831 F1 score: 0.9831 Precision: 0.9831 Recall: 0.9831 \n",
            "val Loss: 0.1463 Acc: 0.8990 F1 score: 0.8990 Precision: 0.8991 Recall: 0.8991 \n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0232 Acc: 0.9810 F1 score: 0.9810 Precision: 0.9810 Recall: 0.9810 \n",
            "val Loss: 0.1483 Acc: 0.8975 F1 score: 0.8975 Precision: 0.8977 Recall: 0.8977 \n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0231 Acc: 0.9801 F1 score: 0.9801 Precision: 0.9801 Recall: 0.9801 \n",
            "val Loss: 0.1499 Acc: 0.8965 F1 score: 0.8965 Precision: 0.8969 Recall: 0.8967 \n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.0213 Acc: 0.9826 F1 score: 0.9826 Precision: 0.9826 Recall: 0.9826 \n",
            "val Loss: 0.1468 Acc: 0.8960 F1 score: 0.8960 Precision: 0.8961 Recall: 0.8961 \n",
            "\n",
            "Training complete in 0m 39s\n",
            "Best val F1: 0.899999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6gcN4AzVgq6"
      },
      "source": [
        "We managed to get 0.8999 F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDAY1-WcVo8R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}